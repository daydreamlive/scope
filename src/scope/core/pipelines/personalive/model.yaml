# PersonaLive model configuration
# Based on https://github.com/GVCLab/PersonaLive

# Base model paths (relative to models_dir)
pretrained_base_model_path: "sd-image-variations-diffusers"
image_encoder_path: "sd-image-variations-diffusers/image_encoder"
vae_model_path: "sd-vae-ft-mse"

# PersonaLive custom weights
personalive_weights_path: "PersonaLive/pretrained_weights/personalive"

# TensorRT acceleration (optional)
# Run `convert-personalive-trt` to generate the engine file
# Engine path format: tensorrt/unet_work_{height}x{width}.engine
tensorrt:
  onnx_path: "PersonaLive/pretrained_weights/onnx/unet_opt/unet_opt.onnx"
  engine_dir: "PersonaLive/pretrained_weights/tensorrt"

# Inference configuration
dtype: "fp16"
batch_size: 1
temporal_window_size: 4
temporal_adaptive_step: 4
num_inference_steps: 4
seed: 42

# Resolution
reference_image_height: 512
reference_image_width: 512
output_height: 512
output_width: 512

# Scheduler configuration
noise_scheduler_kwargs:
  beta_start: 0.00085
  beta_end: 0.02
  beta_schedule: "scaled_linear"
  clip_sample: false
  steps_offset: 1
  prediction_type: "epsilon"
  timestep_spacing: "trailing"

# UNet additional kwargs for 3D UNet
unet_additional_kwargs:
  use_inflated_groupnorm: true
  unet_use_cross_frame_attention: false
  unet_use_temporal_attention: false
  use_motion_module: true
  motion_module_resolutions:
    - 1
    - 2
    - 4
    - 8
  motion_module_mid_block: true
  motion_module_decoder_only: false
  motion_module_type: "Vanilla"
  motion_module_kwargs:
    num_attention_heads: 8
    num_transformer_block: 1
    cross_attention_dim: 16
    attention_block_types:
      - "Spatial_Cross"
      - "Spatial_Cross"
    temporal_position_encoding: false
    temporal_position_encoding_max_len: 32
    temporal_attention_dim_div: 1
  use_temporal_module: true
  temporal_module_type: "Vanilla"
  temporal_module_kwargs:
    num_attention_heads: 8
    num_transformer_block: 1
    attention_block_types:
      - "Temporal_Self"
      - "Temporal_Self"
    temporal_position_encoding: true
    temporal_position_encoding_max_len: 32
    temporal_attention_dim_div: 1
