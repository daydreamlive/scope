# Reward-Forcing Pipeline Configuration
# Based on https://github.com/JaydenLu666/Reward-Forcing
#
# Reward-Forcing is a training method that enables few-step video generation
# by learning from reward signals. The distilled model can generate high-quality
# videos in just 4 denoising steps (vs. 50+ for standard diffusion).
#
# Key features:
# - EMA sink mechanism for real-time streaming with bounded memory
# - 4-step denoising schedule for fast inference
# - Based on Wan2.1-T2V-1.3B architecture

# Base model configuration
base_model_name: Wan2.1-T2V-1.3B
base_model_kwargs:
  timestep_shift: 5.0
  # Sink size: number of frames to keep as semantic anchors
  # More sink tokens = stronger semantic preservation, but uses more memory
  # Original Reward-Forcing uses 3, increase to 6 for better anti-drift
  sink_size: 3
  # EMA coefficient for sink token compression
  # Higher values (e.g., 0.999) = slower update, more stable long-term context
  # Lower values (e.g., 0.9) = faster update, more recent context emphasis
  # IMPORTANT: Original Reward-Forcing was trained with implicit 0.999
  compression_alpha: 0.999

generator_model_name: "generator"

# Frame and patch configuration
num_frame_per_block: 3
# IMPORTANT: Original Reward-Forcing uses local_attn_size=9, not 12!
local_attn_size: 9
vae_spatial_downsample_factor: 8
vae_temporal_downsample_factor: 4
patch_embedding_spatial_downsample_factor: 2
max_rope_freq_table_seq_len: 1024

# Inference configuration (4-step denoising)
# CRITICAL: warp_denoising_step must be true to match training!
# This transforms [1000, 750, 500, 250] through scheduler's timestep table
warp_denoising_step: true
denoising_step_list:
  - 1000
  - 750
  - 500
  - 250

# Context noise: noise level for clean KV cache update (0 = no noise)
context_noise: 0
