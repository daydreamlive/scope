# =============================================================================
# CUDA 13.0 variant of pyproject.toml
# Use with Dockerfile.cu130 for smaller image size (~71% smaller CUDA binaries)
# Note: Drops Volta GPU support (V100), requires Turing or newer
# =============================================================================

[project]
name = "daydream-scope"
version = "0.1.3"
description = "A tool for running and customizing real-time, interactive generative AI pipelines and models"
readme = "README.md"
requires-python = ">=3.12"
authors = [
    {name = "Yondon Fu", email = "yondon@livepeer.org"},
    {name = "Rafal Leszko", email = "rafal@livepeer.org"}
]
maintainers = [
    {name = "Yondon Fu", email = "yondon@livepeer.org"},
    {name = "Rafal Leszko", email = "rafal@livepeer.org"}
]
keywords = ["ai", "video", "streaming", "webrtc", "diffusion", "real-time"]
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Intended Audience :: Science/Research",
    "Intended Audience :: End Users/Desktop",
    "License :: OSI Approved :: MIT License",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
    "Topic :: Multimedia :: Video",
    "Topic :: Software Development :: Libraries :: Python Modules",
]
dependencies = [
    "aiortc>=1.13.0",
    "aiohttp>=3.9.0",
    "fastapi>=0.116.1",
    "httpx>=0.28.1",
    "twilio>=9.8.0",
    "uvicorn>=0.35.0",
    "torch==2.9.1",
    "torchvision==0.24.1",
    "easydict>=1.13",
    "diffusers>=0.31.0",
    "ftfy>=6.3.1",
    "transformers>=4.49.0",
    "einops>=0.8.1",
    "lmdb>=1.7.3",
    "omegaconf>=2.3.0",
    "accelerate>=1.1.1",
    "flash-attn==2.8.3; sys_platform == 'linux'",
    "sageattention==2.2.0; sys_platform == 'linux'",
    "safetensors>=0.6.2",
    "huggingface_hub>=0.25.0",
    "pluggy>=1.5.0",
    "click>=8.3.1",
    "peft>=0.17.1",
    "torchao==0.15.0",
    "kernels>=0.10.4",
    "triton==3.5.1; sys_platform == 'linux'",
]

[project.optional-dependencies]
kafka = [
    "aiokafka>=0.10.0",
]

[project.scripts]
daydream-scope = "scope.server.app:main"
build = "scope.server.build:main"
download_models = "scope.server.download_models:main"

[project.urls]
Homepage = "https://github.com/daydreamlive/scope"
Repository = "https://github.com/daydreamlive/scope"
Issues = "https://github.com/daydreamlive/scope/issues"

[tool.uv]
preview = true
# Override cuDNN version to fix PyTorch 2.9.1 Conv3D bf16 performance regression
# See: https://github.com/pytorch/pytorch/issues/168167
override-dependencies = [
    "nvidia-cudnn-cu12>=9.15; sys_platform == 'linux'",
]

[tool.uv.extra-build-dependencies]
flash-attn = [{ requirement = "torch", match-runtime = true, marker = "sys_platform == 'linux'" }]

[tool.uv.extra-build-variables]
flash-attn = { FLASH_ATTENTION_SKIP_CUDA_BUILD = "TRUE" }

[tool.uv.sources]
# CUDA 13.0 PyTorch index
torch = [
    { index = "pytorch-cu130", marker = "sys_platform == 'linux'" },
]
torchvision = [
    { index = "pytorch-cu130", marker = "sys_platform == 'linux'" },
]
flash-attn = [
    # Prebuilt Linux wheels for CUDA 13.0 from https://github.com/mjun0812/flash-attention-prebuild-wheels
    { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.16/flash_attn-2.8.3+cu130torch2.9cxx11abiTRUE-cp312-cp312-linux_x86_64.whl", marker = "sys_platform == 'linux'" },
]
sageattention = [
    # Prebuilt Linux wheels for CUDA 13.0 from HuggingFace
    { url = "https://huggingface.co/vjump21848/sageattention-pre-compiled-wheel/resolve/main/sageattention-2.2.0+cu130-cp312-cp312-linux_x86_64.whl", marker = "sys_platform == 'linux'" },
]

[[tool.uv.index]]
name = "pytorch-cu130"
url = "https://download.pytorch.org/whl/cu130"
explicit = true

[dependency-groups]
dev = [
    "imageio>=2.37.0",
    "imageio-ffmpeg>=0.6.0",
    "ruff==0.14.11",
    "pre-commit>=4.0.0",
    "twine>=5.0.0",
    "pytest>=8.4.2",
    "freezegun>=1.5.5",
]

[tool.pytest.ini_options]
testpaths = ["tests"]

[tool.ruff]
line-length = 88
target-version = "py312"
exclude = [
    "*/vendor/*",
    "**/vendor/**",
    "*/modules/*",
    "**/modules/**",
    ".venv",
    "__pycache__",
]

[tool.ruff.lint]
select = [
    "E",  # pycodestyle errors
    "W",  # pycodestyle warnings
    "F",  # pyflakes
    "I",  # isort
    "B",  # flake8-bugbear
    "C4", # flake8-comprehensions
    "UP", # pyupgrade
]
ignore = [
    "E501", # line too long, handled by formatter
    "B008", # function call in argument defaults (FastAPI Depends pattern)
]

[tool.ruff.format]
quote-style = "double"
indent-style = "space"
skip-magic-trailing-comma = false
line-ending = "auto"

[tool.ruff.lint.isort]
known-first-party = ["src"]

[build-system]
requires = ["hatchling", "editables"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src/scope"]

[tool.hatch.build.targets.wheel.force-include]
# Include .pth file for automatic cuDNN patching at Python startup
"patches.pth" = "patches.pth"

[tool.hatch.build]
include = [
    "frontend/dist/assets/**/*",
    "frontend/dist/index.html",
    "./README.md",
    "./LICENSE.md",
    "*.toml",
    ".python-version",
    "./*.yaml",
    "./*.yml",
    "./*.json",
]

exclude = [
    "__pycache__/**/*",
    "*.pyc",
    "*.pyo",
    "*.pyd",
    ".pytest_cache/**/*",
    ".coverage",
    "htmlcov/**/*",
    ".tox/**/*",
    ".venv/**/*",
    "venv/**/*",
    "env/**/*",
    ".env",
    ".env.*",
    "*.egg-info/**/*",
    "dist/**/*",
    "build/**/*",
    "models/**/*",
]
